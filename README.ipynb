{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.functions.preprocessing as pp\n",
    "from scripts.functions.metadata import metadata_longform_papio, meta_papio\n",
    "from scripts.functions.hypermodel import WinWavTransferLearning\n",
    "from scripts.functions.segmentation import segmentation, df_pred, wav_creation\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here an exemple to replicate the model used in the paper https://arxiv.org/abs/2302.07640\n",
    "\n",
    "The labeled data, as well as the longform audio recordings used as exemple, and all the vocalizations extracted during the month, are available at https://doi.org/10.5281/zenodo.7963124\n",
    "\n",
    "The folder structure should be as follows: in the working directory,   \n",
    "- a folder *data*. In this folder,\n",
    "    - a folder *vocalizations*, in which we have the labeled data,\n",
    "    - a folder *data_augmentation*, in which we save the modified vocalizations,\n",
    "        - a folder *test*, for the modified recordings of the test partition,\n",
    "        - a folder *train*, for the modified recordings of the train partition,\n",
    "        - a folder *val*, for the modified recordings of the validation partition.  \n",
    "        For each one, there are three folders, one per condition of modification of the original recordings:\n",
    "            - a folder *noise*,\n",
    "            - a folder *tonality*,\n",
    "            - a folder *speed*,\n",
    "    - a folder *longform_recordings*, in which we have the long form audio recordings,\n",
    "    - a folder *segmentation_papio*, in which we will save the vocalizations detected.\n",
    "- a folder *scripts*. In this folder,\n",
    "    - a folder *functions*, in which we have all the functions used during the pipeline\n",
    "    - a folder *learning*, for the scripts used for the training of the models presentated in the paper,\n",
    "    - a folder *prediction*, for the scripts used for the prediction of all the long form audio recordings presented in the paper, that ouputs the data of the baboon and baby vocalizations.\n",
    "    \n",
    "The gitlab repository is already organized that way. The data available on zenodo are organized that way. The libraries to install can be found in the document requirements.txt   \n",
    "\n",
    "We present here an exemple to replicate the model using a subset of the baboon recordings. We cannot provide all the month for legal reasons. We put 2 hours accessible, as well as the labeled data set used in the paper. We show how to train a model from the labeled dataset and how to use it for the segmentation of these two hours.   \n",
    "The total output of the segmentation of the month is available on zenodo.   \n",
    "\n",
    "For the baby data, none of the long form audio recordings are available, nor the output of the segmentation, for leagal reasons. The labeled dataset is BabbleCor and can be found here https://osf.io/rz4tx/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "We start loading the metadata of the labeled data for the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train, meta_val, meta_test = meta_papio(os.getcwd(), data_augmentation=False, weighting_sampling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we prepare the data. This is done through the creation of a dataset, using the metadata of the labeled data. \n",
    "The recordings are loaded and resampled at 16 KhZ, in mono. \n",
    "We create independant frames through an 80% overlapping 1-second window. \n",
    "We use a resampling strategy to have an uniform distribution among classes during the learning. \n",
    "Because we use transfer-learning from YamNet, the frames are mapped to a log-mel spectrogram.\n",
    "Data augmentation is done before, not during the learning, because we do not expect to have so much labeled data. Thus, we can gain time during the learning without being too expensive in term of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, steps_per_epoch = pp.preparation_data_set(meta_train, resample=True, batch_size=32, transfer_learning=True)\n",
    "val = pp.preparation_data_set(meta_val, resample=False, batch_size=32, transfer_learning=True)\n",
    "\n",
    "input_shape = next(iter(train.unbatch()))[0].shape\n",
    "\n",
    "train = train.shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the hypermodel and the hyperparameters, as well as the callbacks.\n",
    "The values set here are for the exemple and can be changed and increase for a \"true\" learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = winWavTransferLearning(input_shape=input_shape, n_labels=6)\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "tuner = kt.tuners.bayesian.BayesianOptimization(hypermodel=hypermodel,\n",
    "                                               hyperparameters=hp,\n",
    "                                               objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "                                                # increase to have more searching iterations. \n",
    "                                                # Set to 2 here for the exemple\n",
    "                                                max_trials=2,\n",
    "                                                num_initial_points=1,\n",
    "                                                tune_new_entries=True,\n",
    "                                                project_name=\"exemple\")\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, \n",
    "                                             patience=5, verbose=1, \n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(os.getcwd(), \"exemple/cp.hdf5\"),\n",
    "                                               monitor=\"val_loss\", mode=\"min\", \n",
    "                                                save_best_only=True, verbose=1)\n",
    "\n",
    "history = tf.keras.callbacks.CSVLogger(os.path.join(os.getcwd(), \"exemple/train.csv\"),\n",
    "                                      separator=\",\", append=False)\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2,\n",
    "                                                  patience=2, verbose=1)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, history, lr_schedule]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AutoShard\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "train = train.with_options(options)\n",
    "val = val.with_options(options)\n",
    "\n",
    "print(\"Start of the learning\")\n",
    "start = datetime.now()\n",
    "tuner.search(train, epochs=20, validation_data=val, callbacks=callbacks,\n",
    "            steps_per_epoch=steps_per_epoch)\n",
    "delta_training = datetime.now() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "The model has been learned on the labeled data and now can be used to detect vocalizations in long form audio recordings.\n",
    "We start loading the metadata of the longform audio recordings as well as their length and we prepare the data to be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_longform = metadata_longform_papio(os.path.join(os.getcwd(), \"data/longform_recordings\"), \n",
    "                                       length=True)\n",
    "\n",
    "ds = pp.preparation_longform_papio(meta_longform, batch_size=32, transfer_learning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the best model of the optimization process and we use it to find the segments of vocalizations in the recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models()[0]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "y = model.predict(ds)\n",
    "delta_pred = datetime.now() - start\n",
    "print(\"Duration prediction:\", delta_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "Once we learned the model and used it to find the segments of vocalizations in the longform audio recordings, we extract the information.\n",
    "First, we create txt files, one per recordings, in which we have the number of vocalizations found with the time in the recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation(meta_longform, y, baby=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we take the information that we have in the txt files to create the wav files, one per vocalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_creation(baby=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe in which we have more information for each vocalization the model detected (the day, the hour, the duration, the probability of each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pred(meta_longform, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
